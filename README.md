# Data Pipeline: MongoDB â†’ Lakehouse (Medallion Architecture)

Este projeto demonstra a construÃ§Ã£o de um **pipeline de dados endâ€‘toâ€‘end**, integrando uma fonte **NoSQL (MongoDB Atlas)** a um ambiente analÃ­tico moderno, com **orquestraÃ§Ã£o via Apache Airflow**, **ingestÃ£o com Airbyte** e **camadas Medallion (Bronze/Silver/Gold)**. O foco estÃ¡ em **qualidade de dados, escalabilidade e governanÃ§a**.

> Projeto apresentado em um post no LinkedIn (link nos comentÃ¡rios do post).

---

## ðŸš€ Tecnologias Utilizadas
- **IngestÃ£o (EL):** Airbyte Cloud
- **OrquestraÃ§Ã£o:** Apache Airflow (Docker)
- **Fonte:** MongoDB Atlas (Replica Set)
- **Camada Relacional (Silver):** Supabase / PostgreSQL
- **Processamento AnalÃ­tico:** SQL (PostgreSQL / JSONB)
- **Analytics:** Apache Zeppelin

---

## ðŸ“¥ IngestÃ£o de Dados (Airbyte)
Para a fase de extraÃ§Ã£o e carga (EL), foi utilizado o **Airbyte Cloud**:
- **Source:** MongoDB Atlas (Replica Set).
- **Destination:** PostgreSQL no Supabase.
- **Sync Mode:** Incremental Append + Dedup *(Full Refresh utilizado neste laboratÃ³rio)*.
- **PersistÃªncia:** Documentos NoSQL mapeados para colunas `JSONB` na tabela `raw_movies`, preservando a estrutura original para transformaÃ§Ãµes posteriores.

---

## âš™ï¸ OrquestraÃ§Ã£o e TransformaÃ§Ã£o (Airflow)
- **AutomaÃ§Ã£o:** DAGs em Python para controlar a execuÃ§Ã£o das transformaÃ§Ãµes.
- **TransformaÃ§Ã£o (Silver):** Limpeza, padronizaÃ§Ã£o e tipagem de dados.
- **SQL Moderno:** Uso de `CASE WHEN`, `CAST` e operadores JSON (`->`, `->>`) para converter estruturas JSON complexas em colunas relacionais (`FLOAT`, `INT`, `TEXT`).

---

## ðŸ—ï¸ Arquitetura do Projeto
O pipeline segue os princÃ­pios da **Medallion Architecture**, com uma adaptaÃ§Ã£o hÃ­brida:

- **Bronze (Raw):** IngestÃ£o bruta de documentos JSON do MongoDB em tabelas `raw_` no PostgreSQL.
- **Silver (Clean):** TransformaÃ§Ãµes e tipagem via Airflow, extraindo campos JSONB para colunas relacionais no PostgreSQL.
- **Gold (Analytics):** Consumo analÃ­tico e visualizaÃ§Ã£o no Apache Zeppelin.

> ObservaÃ§Ã£o: a arquitetura Medallion foi aplicada de forma hÃ­brida, utilizando PostgreSQL como camada intermediÃ¡ria antes do consumo analÃ­tico.

---

## ðŸ› ï¸ Desafios TÃ©cnicos Superados

### 1ï¸âƒ£ AutenticaÃ§Ã£o SCRAMâ€‘SHAâ€‘256 (Authentication Type 10)
Ao conectar ferramentas locais ao Supabase, foi identificada uma incompatibilidade de handshake JDBC.

**SoluÃ§Ã£o:**
- AtualizaÃ§Ã£o do driver JDBC para a versÃ£o **42.5.4**.
- Ajuste da string de conexÃ£o com parÃ¢metros corretos de **tenant/project ID**.

### 2ï¸âƒ£ Tratamento de Qualidade de Dados (Null Treatment)
Alguns campos vinham como **strings vazias** a partir da fonte NoSQL, quebrando conversÃµes numÃ©ricas.

**SoluÃ§Ã£o:**
ImplementaÃ§Ã£o de lÃ³gica defensiva em SQL para garantir conversÃµes seguras:

```sql
CASE
    WHEN (imdb->>'rating') = '' THEN NULL
    ELSE (imdb->>'rating')::float
END AS rating
```

---

## ðŸ“Š Resultados
- **Conectividade Cloudâ€‘toâ€‘Cloud:** IngestÃ£o bemâ€‘sucedida do **MongoDB Atlas** para o **PostgreSQL no Supabase**.
- **Mapeamento NoSQL â†’ Relacional:** PersistÃªncia de dados em `JSONB` com posterior normalizaÃ§Ã£o.
- **Pipeline Automatizado:** DAGs do Airflow executando transformaÃ§Ãµes de forma reprodutÃ­vel.
- **Dados Prontos para AnÃ¡lise:** Estrutura limpa e tipada para consumo analÃ­tico.
- **Insights Gerados:** Dashboards funcionais no Apache Zeppelin.

<p align="center">
  <img src="images/Airbyte.jpg" alt="Airbyte" width="600" />
  <img src="images/Airflow.jpg" alt="Airflow" width="600" />
  <img src="images/Supabase-dados-limpos.jpg" alt="Supabase" width="600" />
  <img src="images/zeppelin.jpg" alt="Apache Zeppelin" width="600" />
</p>

---

## ðŸ”® PrÃ³ximos Passos
- Evoluir a camada **Gold** para um **Lakehouse com Delta Lake**.
- Implementar **versionamento de dados (Time Travel)**.
- Adicionar **testes de qualidade de dados** e **observabilidade**.

---

## ðŸ‘¤ Autor
Raphael Rugna

Engenharia de Dados | Big Data | Airflow | Databricks | SQL | Python

