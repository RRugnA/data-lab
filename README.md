# Data Pipeline: MongoDB â†’ Lakehouse (Medallion Architecture)

Este projeto demonstra a construÃ§Ã£o de um **pipeline de dados endâ€‘toâ€‘end**, integrando uma fonte **NoSQL (MongoDB Atlas)** a um ambiente analÃ­tico moderno, com **orquestraÃ§Ã£o via Apache Airflow**, **ingestÃ£o com Airbyte** e **camadas Medallion (Bronze/Silver/Gold)**. O foco estÃ¡ em **qualidade de dados, escalabilidade e governanÃ§a**.

> Projeto apresentado em um post no LinkedIn (link nos comentÃ¡rios do post).

---

## ğŸš€ Tecnologias Utilizadas
- **IngestÃ£o (EL):** Airbyte Cloud
- **OrquestraÃ§Ã£o:** Apache Airflow (Docker)
- **Fonte:** MongoDB Atlas (Replica Set)
- **Camada Relacional (Silver):** Supabase / PostgreSQL
- **Processamento AnalÃ­tico:** SQL (PostgreSQL / JSONB)
- **Analytics:** Apache Zeppelin

---

## ğŸ“¥ IngestÃ£o de Dados (Airbyte)
Para a fase de extraÃ§Ã£o e carga (EL), foi utilizado o **Airbyte Cloud**:
- **Source:** MongoDB Atlas (Replica Set).
- **Destination:** PostgreSQL no Supabase.
- **Sync Mode:** Incremental Append + Dedup *(Full Refresh utilizado neste laboratÃ³rio)*.
- **PersistÃªncia:** Documentos NoSQL mapeados para colunas `JSONB` na tabela `raw_movies`, preservando a estrutura original para transformaÃ§Ãµes posteriores.

---

## âš™ï¸ OrquestraÃ§Ã£o e TransformaÃ§Ã£o (Airflow)
- **AutomaÃ§Ã£o:** DAGs em Python para controlar a execuÃ§Ã£o das transformaÃ§Ãµes.
- **TransformaÃ§Ã£o (Silver):** Limpeza, padronizaÃ§Ã£o e tipagem de dados.
- **SQL Moderno:** Uso de `CASE WHEN`, `CAST` e operadores JSON (`->`, `->>`) para converter estruturas JSON complexas em colunas relacionais (`FLOAT`, `INT`, `TEXT`).

---

## ğŸ—ï¸ Arquitetura do Projeto
O pipeline segue os princÃ­pios da **Medallion Architecture**, com uma adaptaÃ§Ã£o hÃ­brida:

- **Bronze (Raw):** IngestÃ£o bruta de documentos JSON do MongoDB em tabelas `raw_` no PostgreSQL.
- **Silver (Clean):** TransformaÃ§Ãµes e tipagem via Airflow, extraindo campos JSONB para colunas relacionais no PostgreSQL.
- **Gold (Analytics):** Consumo analÃ­tico e visualizaÃ§Ã£o no Apache Zeppelin.

> ObservaÃ§Ã£o: a arquitetura Medallion foi aplicada de forma hÃ­brida, utilizando PostgreSQL como camada intermediÃ¡ria antes do consumo analÃ­tico.

---

## ğŸ› ï¸ Desafios TÃ©cnicos Superados

### 1ï¸âƒ£ AutenticaÃ§Ã£o SCRAMâ€‘SHAâ€‘256 (Authentication Type 10)
Ao conectar ferramentas locais ao Supabase, foi identificada uma incompatibilidade de handshake JDBC.

**SoluÃ§Ã£o:**
- AtualizaÃ§Ã£o do driver JDBC para a versÃ£o **42.5.4**.
- Ajuste da string de conexÃ£o com parÃ¢metros corretos de **tenant/project ID**.

### 2ï¸âƒ£ Tratamento de Qualidade de Dados (Null Treatment)
Alguns campos vinham como **strings vazias** a partir da fonte NoSQL, quebrando conversÃµes numÃ©ricas.

**SoluÃ§Ã£o:**
ImplementaÃ§Ã£o de lÃ³gica defensiva em SQL para garantir conversÃµes seguras:

```sql
CASE
    WHEN (imdb->>'rating') = '' THEN NULL
    ELSE (imdb->>'rating')::float
END AS rating
```

---

## â³ Time Travel e Versionamento de Dados (Delta Lake)

Como parte da camada analÃ­tica, foi implementado **Time Travel utilizando Delta Lake**, permitindo:
- Auditoria de alteraÃ§Ãµes ao longo do tempo.
- RecuperaÃ§Ã£o de versÃµes histÃ³ricas dos dados.
- ComparaÃ§Ã£o entre estados antigos e atuais das tabelas analÃ­ticas.

A funcionalidade foi validada por meio de consultas a versÃµes especÃ­ficas da tabela, garantindo rastreabilidade e confiabilidade do dado â€” um requisito essencial em ambientes de dados governados.

> Exemplo de uso: consultas a versÃµes anteriores para anÃ¡lise de regressÃ£o de mÃ©tricas e validaÃ§Ã£o de correÃ§Ãµes de dados.

---

## ğŸ“Š Resultados
- **Conectividade Cloudâ€‘toâ€‘Cloud:** IngestÃ£o bemâ€‘sucedida do **MongoDB Atlas** para o **PostgreSQL no Supabase**.
- **Mapeamento NoSQL â†’ Relacional:** PersistÃªncia de dados em `JSONB` com posterior normalizaÃ§Ã£o.
- **Pipeline Automatizado:** DAGs do Airflow executando transformaÃ§Ãµes de forma reprodutÃ­vel.
- **Dados Prontos para AnÃ¡lise:** Estrutura limpa e tipada para consumo analÃ­tico.
- **Insights Gerados:** Dashboards funcionais no Apache Zeppelin.

<p align="center">
  <img src="images/Airbyte.jpg" alt="Airbyte" width="600" />
  <img src="images/Airflow 2.jpg" alt="Airflow" width="600" />
  <img src="images/Databricks-Describe-History.jpg" alt="Databricks" width="600" />
  <img src="images/zeppelin.jpg" alt="Apache Zeppelin" width="600" />
</p>

---

## ğŸ”® PrÃ³ximos Passos
- Expandir o uso de **Time Travel** para cenÃ¡rios de rollback automatizado.
- Implementar **testes de qualidade de dados** (Great Expectations / Soda).
- Adicionar **observabilidade do pipeline** (SLAs, alertas e mÃ©tricas).

